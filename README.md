<div align="center">ğŸ“£ FluentIQ</div>
<div align="center">AI-Powered Multimodal Public Speaking & Communication Feedback System</div>
<div align="center"> A research-driven project analyzing **speech**, **language**, and **non-verbal communication** using multimodal machine learning. <br><br> <img src="https://img.shields.io/badge/Framework-FastAPI-009688?style=flat-square"/> <img src="https://img.shields.io/badge/Frontend-HTML%2FCSS%2FJS-blue?style=flat-square"/> <img src="https://img.shields.io/badge/Models-ML%2FRL%2BRule--Based-orange?style=flat-square"/> </div>
ğŸ“ Overview

FluentIQ is an AI-powered multimodal analysis system designed to evaluate key aspects of public speaking performance using:

ğŸ”¹ Audio cues (fluency, pauses, rate of speech)
ğŸ”¹ Textual language quality (grammar, coherence, readability)
ğŸ”¹ Visual cues (posture, gaze, movement)
ğŸ”¹ Fused multimodal scoring (overall performance score)

It provides:

Detailed feedback

Skill improvement plans

Analytics dashboard

Session-wise history

Radar & line charts

Export options (CSV, PNG, JSON)

This project supports research titled:
â€œAI-Powered Multimodal Framework for Enhancing Public Speaking and Communication Skills.â€

ğŸ§  System Architecture
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Frontend    â”‚
               â”‚ HTML/CSS/JS   â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Upload video/audio
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     FastAPI API     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼             â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio     â”‚â†’ â”‚ Text      â”‚â†’â”‚ Video       â”‚â†’â”‚ Fusion Model  â”‚
â”‚ Analysis  â”‚  â”‚ Analysis  â”‚ â”‚ Analysis    â”‚ â”‚ (Rule-based)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ SQLite Session Storage â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ History Dashboard (Charts) â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ¨ Features
ğŸ¤ Audio Analysis

Filler-word detection

Pause duration

Speech rate

Word count

Whisper-based transcript extraction (optional: API-based)

ğŸ“ Text Analysis

Grammar scoring (LanguageTool)

Coherence & readability scoring (Flesch-Kincaid, rule-based)

Highlighted feedback with real suggestions

ğŸ¥ Video Analysis

Face detection

Gaze stability

Posture alignment

Movement intensity

(Lightweight rule-based version implemented â€” replaceable with ML models later.)

ğŸ§© Multimodal Fusion

Scores combined using weighted fusion:

40% language

40% audio

20% video

ğŸ“Š Analytics Dashboard

Line chart (performance over time)

Radar chart (session comparison)

Session history table

Transcript viewer

CSV / PNG / JSON exports

ğŸŒ Modern Frontend

Upload UI

Progress indicator

Result view with charts

Clean dashboard design

âš™ï¸ Tech Stack
Layer	Technology
Backend	FastAPI, Python 3.12
Audio	Librosa, rule-based metrics
Text	LanguageTool, readability index
Video	OpenCV
Storage	SQLite
Frontend	HTML, CSS, Vanilla JS
Charts	Chart.js
Export	CSV, PNG, JSON
ğŸš€ Setup Instructions
1. Clone the repository
git clone https://github.com/YOUR_USERNAME/fluentiq-multimodal.git
cd fluentiq-multimodal/backend

2. Setup Python virtual environment
python3 -m venv .venv
source .venv/bin/activate

3. Install dependencies
pip install -r requirements.txt


If you donâ€™t have the file yet, create one:

fastapi
uvicorn[standard]
python-multipart
language_tool_python
opencv-python
numpy
librosa

4. Run the backend
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000


Test:
ğŸ”— http://127.0.0.1:8000/ping

ğŸ”— http://127.0.0.1:8000/docs

5. Run the frontend

From the frontend/ folder:

Option A â€” VS Code Live Server

Right-click â†’ Open with Live Server

Option B â€” Python static server
python3 -m http.server 5500


Then open:
ğŸ”— http://127.0.0.1:5500/pages/index.html

ğŸ§ª API Endpoints
Health Check
GET /ping

Multimodal Analysis
POST /analyze/audio
file: UploadFile


Returns:

{
  "transcript": "...",
  "audio": {...},
  "text": {...},
  "video": {...},
  "fused": {...},
  "stats": {...}
}

History
GET /history/all
GET /history/summary

ğŸ“ˆ Dashboard Features

Compare two sessions (radar)

View overall trajectory (line chart)

Inspect transcripts

Export JSON of any session

Export all sessions as CSV

ğŸ“„ Citation (research paper)

If you use this system in academic work:

Gundu, Uday. *AI-Powered Multimodal Framework for Enhancing Public Speaking and Communication Skills*. 2025.

ğŸ¤ Contributing

Pull requests are welcome!
Suggestions for improving models (especially video posture or multimodal fusion) are encouraged.

â­ Show your support

If this project helps your work or research, star the repository!